{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# \ud83d\udd78\ufe0f Network Analysis of Python Package Vulnerabilities\n\nAn\u00e1lise de vulnerabilidades usando **Teoria de Redes**:\n\n- **Temporal Analysis**: evolu\u00e7\u00e3o, survival curves, vulnerabilidades ativas\n- **Network Metrics**: grau, betweenness, eigenvector, modularidade\n- **Dependency Analysis**: exposi\u00e7\u00e3o direta/indireta, profundidade\n- **Comparison**: General vs AI Libraries"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom lifelines import KaplanMeierFitter\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('husl')\nplt.rcParams['figure.figsize'] = (14, 6)\n\nprint('\u2705 Setup complete')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83d\udcc2 1. Load Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load\nvulns = pd.read_csv('outputs/top_pypi_snyk_timeline_20231101_20251101.csv')\ndeps = pd.read_csv('python_dependencies_edges.csv')\n\nprint(f'Vulnerabilidades: {len(vulns)}, Pacotes: {vulns[\"package\"].nunique()}')\nprint(f'Depend\u00eancias: {len(deps)} arestas')\n\n# Parse dates\nfor col in ['first_affected_date', 'disclosed_date', 'mitigation_date']:\n    if col in vulns.columns:\n        vulns[col] = pd.to_datetime(vulns[col], errors='coerce')\n\n# Severity numeric\nseverity_map = {'low': 1, 'moderate': 2, 'medium': 2, 'high': 3, 'critical': 4, 'unknown': 2}\nvulns['severity_norm'] = vulns['severity'].str.lower().map(severity_map).fillna(2)\n\nvulns.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83e\udd16 2. AI Libraries"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "AI_LIBS = {\n    'torch', 'tensorflow', 'keras', 'pytorch-lightning', 'lightning',\n    'scikit-learn', 'xgboost', 'lightgbm', 'catboost',\n    'transformers', 'sentence-transformers', 'tokenizers',\n    'langchain', 'langchain-core', 'langchain-community',\n    'llama-index', 'llama-index-core',\n    'vllm', 'litellm', 'openai',\n    'mlflow', 'wandb', 'clearml', 'sagemaker',\n    'gradio', 'streamlit',\n    'opencv-python', 'pillow'\n}\n\nvulns['is_ai_lib'] = vulns['package'].str.lower().isin(AI_LIBS)\nprint(f'AI vulnerabilities: {vulns[\"is_ai_lib\"].sum()}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83d\udd78\ufe0f 3. Build Network"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "G = nx.DiGraph()\n\nfor _, row in deps.iterrows():\n    src, tgt = str(row['source']).lower(), str(row['target']).lower()\n    if src and tgt and src != 'nan' and tgt != 'nan':\n        G.add_edge(src, tgt)\n\nfor pkg in vulns['package'].unique():\n    if pkg and str(pkg) != 'nan':\n        G.add_node(str(pkg).lower())\n\nprint(f'N\u00f3s: {G.number_of_nodes()}, Arestas: {G.number_of_edges()}')\nprint(f'Densidade: {nx.density(G):.6f}')\n\nUG = G.to_undirected()\nprint(f'Componentes: {nx.number_connected_components(UG)}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83d\udcca 4. Network Metrics"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Calculando m\u00e9tricas...')\n\ndegree_dict = dict(G.degree())\nin_degree_dict = dict(G.in_degree())\nout_degree_dict = dict(G.out_degree())\n\nprint('  Betweenness...')\nbetweenness_dict = nx.betweenness_centrality(G)\n\nprint('  Eigenvector...')\ntry:\n    eigenvector_dict = nx.eigenvector_centrality_numpy(UG, max_iter=1000)\nexcept:\n    eigenvector_dict = {}\n\nprint('  PageRank...')\npagerank_dict = nx.pagerank(G)\n\nmetrics_df = pd.DataFrame({\n    'package': list(G.nodes()),\n    'degree': [degree_dict.get(n, 0) for n in G.nodes()],\n    'in_degree': [in_degree_dict.get(n, 0) for n in G.nodes()],\n    'out_degree': [out_degree_dict.get(n, 0) for n in G.nodes()],\n    'betweenness': [betweenness_dict.get(n, 0) for n in G.nodes()],\n    'eigenvector': [eigenvector_dict.get(n, 0) for n in G.nodes()],\n    'pagerank': [pagerank_dict.get(n, 0) for n in G.nodes()]\n})\n\nmetrics_df['is_ai_lib'] = metrics_df['package'].isin(AI_LIBS)\n\nprint('\u2705 Done')\nmetrics_df.describe()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83c\udfc6 5. Top Packages"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Top 15 by IN-DEGREE (most depended-upon):')\nmetrics_df.nlargest(15, 'in_degree')[['package', 'in_degree', 'is_ai_lib']]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('Top 15 by BETWEENNESS (connectors):')\nmetrics_df.nlargest(15, 'betweenness')[['package', 'betweenness', 'is_ai_lib']]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83d\udd17 6. Merge with Vulnerabilities"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "vulns['package_lower'] = vulns['package'].str.lower()\n\nvulns_net = vulns.merge(\n    metrics_df, \n    left_on='package_lower', \n    right_on='package', \n    how='left',\n    suffixes=('', '_net')\n)\n\nnet_cols = ['degree', 'in_degree', 'out_degree', 'betweenness', 'eigenvector', 'pagerank']\nvulns_net[net_cols] = vulns_net[net_cols].fillna(0)\n\nprint(f'Merged: {len(vulns_net)} rows')\nvulns_net[['package', 'cve', 'severity', 'degree', 'in_degree']].head(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83d\udcc8 7. TEMPORAL ANALYSIS"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "vulns_net['disclosed_year'] = vulns_net['disclosed_date'].dt.year\n\ntemporal_total = vulns_net.groupby('disclosed_year').size().reset_index(name='total')\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# Total por ano\nax1.bar(temporal_total['disclosed_year'], temporal_total['total'], color='steelblue', edgecolor='black')\nfor _, row in temporal_total.iterrows():\n    ax1.text(row['disclosed_year'], row['total'] + 5, \n             f\"{row['total']}\\n({row['total']/temporal_total['total'].sum()*100:.0f}%)\", \n             ha='center', va='bottom', fontsize=9)\nax1.set_xlabel('Year', fontweight='bold')\nax1.set_ylabel('# Vulnerabilities', fontweight='bold')\nax1.set_title('Temporal Evolution', fontsize=14, fontweight='bold')\nax1.grid(axis='y', alpha=0.3)\n\n# Por severidade\npivot = vulns_net.groupby(['disclosed_year', 'severity']).size().reset_index(name='count')\npivot = pivot.pivot(index='disclosed_year', columns='severity', values='count').fillna(0)\npivot.plot(kind='bar', stacked=True, ax=ax2, color=['green', 'blue', 'orange', 'red', 'gray'])\nax2.set_xlabel('Year', fontweight='bold')\nax2.set_ylabel('# Vulnerabilities', fontweight='bold')\nax2.set_title('By Severity Over Time', fontsize=14, fontweight='bold')\nax2.legend(title='Severity', bbox_to_anchor=(1.05, 1))\n\nplt.tight_layout()\nplt.savefig('outputs/plots/temporal_evolution.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint('\u2705 Saved: outputs/plots/temporal_evolution.png')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \u23f1\ufe0f 8. SURVIVAL ANALYSIS"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "vulns_surv = vulns_net[\n    vulns_net['time_to_fix_from_disclosure_days'].notna() & \n    (vulns_net['time_to_fix_from_disclosure_days'] >= 0)\n].copy()\n\nvulns_surv['event'] = vulns_surv['mitigation_date'].notna().astype(int)\n\nprint(f'For survival: {len(vulns_surv)}')\nprint(f'Fixed: {vulns_surv[\"event\"].sum()}, Not fixed: {(vulns_surv[\"event\"] == 0).sum()}')\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\nkmf = KaplanMeierFitter()\n\n# Por severidade\nax = axes[0]\nfor severity in ['low', 'moderate', 'high', 'critical']:\n    mask = vulns_surv['severity'].str.lower() == severity\n    if mask.sum() > 0:\n        kmf.fit(\n            durations=vulns_surv[mask]['time_to_fix_from_disclosure_days'],\n            event_observed=vulns_surv[mask]['event'],\n            label=severity\n        )\n        kmf.plot_survival_function(ax=ax, ci_show=True)\n\nax.set_xlabel('Delay (days)', fontweight='bold')\nax.set_ylabel('Survival Probability', fontweight='bold')\nax.set_title('Time to Fix by Severity', fontsize=14, fontweight='bold')\nax.legend(title='Severity')\nax.grid(alpha=0.3)\n\n# AI vs Non-AI\nax = axes[1]\nfor is_ai, label in [(True, 'AI'), (False, 'Non-AI')]:\n    mask = vulns_surv['is_ai_lib'] == is_ai\n    if mask.sum() > 0:\n        kmf.fit(\n            durations=vulns_surv[mask]['time_to_fix_from_disclosure_days'],\n            event_observed=vulns_surv[mask]['event'],\n            label=label\n        )\n        kmf.plot_survival_function(ax=ax, ci_show=True)\n\nax.set_xlabel('Delay (days)', fontweight='bold')\nax.set_ylabel('Survival Probability', fontweight='bold')\nax.set_title('AI vs Non-AI', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('outputs/plots/survival_analysis.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint('\u2705 Saved: outputs/plots/survival_analysis.png')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83d\udea8 9. ACTIVE VULNERABILITIES"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "vulns_net['is_active'] = vulns_net['mitigation_date'].isna()\nactive_vulns = vulns_net[vulns_net['is_active']].copy()\n\nprint(f'Active: {len(active_vulns)} ({len(active_vulns)/len(vulns_net)*100:.1f}%)')\nprint(f'AI: {active_vulns[\"is_ai_lib\"].sum()}, Non-AI: {(~active_vulns[\"is_ai_lib\"]).sum()}')\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# Pie chart\nstatus_counts = vulns_net['is_active'].value_counts()\naxes[0].pie(status_counts.values, labels=['Mitigated', 'Active'], autopct='%1.1f%%',\n            colors=['lightgreen', 'lightcoral'], startangle=90)\naxes[0].set_title('Active vs Mitigated', fontsize=14, fontweight='bold')\n\n# By severity\nactive_sev = active_vulns['severity'].value_counts().reindex(['low', 'moderate', 'high', 'critical'], fill_value=0)\naxes[1].bar(range(len(active_sev)), active_sev.values, color=['green', 'blue', 'orange', 'red'], edgecolor='black')\naxes[1].set_xticks(range(len(active_sev)))\naxes[1].set_xticklabels(active_sev.index, rotation=45)\naxes[1].set_ylabel('# Active', fontweight='bold')\naxes[1].set_title('Active by Severity', fontsize=14, fontweight='bold')\n\n# AI vs Non-AI\nactive_ai = active_vulns.groupby(['is_ai_lib', 'severity']).size().unstack(fill_value=0)\nactive_ai.T.plot(kind='bar', stacked=True, ax=axes[2], color=['lightblue', 'lightcoral'])\naxes[2].set_xlabel('Severity', fontweight='bold')\naxes[2].set_ylabel('# Active', fontweight='bold')\naxes[2].set_title('AI vs Non-AI', fontsize=14, fontweight='bold')\naxes[2].legend(['Non-AI', 'AI'])\n\nplt.tight_layout()\nplt.savefig('outputs/plots/active_vulnerabilities.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint('\u2705 Saved: outputs/plots/active_vulnerabilities.png')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83d\udcca 10. COMPARISON: AI vs Non-AI"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "metrics_ai = metrics_df[metrics_df['is_ai_lib']]\nmetrics_non_ai = metrics_df[~metrics_df['is_ai_lib']]\n\ncomparison_metrics = ['degree', 'in_degree', 'betweenness', 'pagerank']\ncomparison_data = []\n\nfor metric in comparison_metrics:\n    stat, pvalue = stats.mannwhitneyu(metrics_ai[metric], metrics_non_ai[metric], alternative='two-sided')\n    comparison_data.append({\n        'metric': metric,\n        'ai_mean': metrics_ai[metric].mean(),\n        'non_ai_mean': metrics_non_ai[metric].mean(),\n        'p_value': pvalue,\n        'sig': '***' if pvalue < 0.001 else '**' if pvalue < 0.01 else '*' if pvalue < 0.05 else 'ns'\n    })\n\ncomparison_df = pd.DataFrame(comparison_data)\ncomparison_df.to_csv('outputs/summaries/network_comparison.csv', index=False)\n\nprint('Statistical Comparison:')\ncomparison_df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\naxes = axes.flatten()\n\nfor idx, metric in enumerate(comparison_metrics):\n    ax = axes[idx]\n    data = [metrics_non_ai[metric], metrics_ai[metric]]\n    bp = ax.boxplot(data, labels=['Non-AI', 'AI'], patch_artist=True, showmeans=True)\n    bp['boxes'][0].set_facecolor('lightblue')\n    bp['boxes'][1].set_facecolor('lightcoral')\n    \n    row = comparison_df[comparison_df['metric'] == metric].iloc[0]\n    ax.text(1.5, ax.get_ylim()[1] * 0.9, f\"p={row['p_value']:.4f} {row['sig']}\",\n            ha='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n    \n    ax.set_ylabel(metric.replace('_', ' ').title(), fontweight='bold')\n    ax.grid(axis='y', alpha=0.3)\n\nplt.suptitle('Network Metrics: AI vs Non-AI', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.savefig('outputs/plots/network_comparison.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint('\u2705 Saved: outputs/plots/network_comparison.png')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83c\udf93 11. SUMMARY"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('='*80)\nprint('SUMMARY')\nprint('='*80)\n\nprint(f'\\nNETWORK:')\nprint(f'  Nodes: {G.number_of_nodes()}')\nprint(f'  Edges: {G.number_of_edges()}')\nprint(f'  Density: {nx.density(G):.6f}')\n\nprint(f'\\nVULNERABILITIES:')\nprint(f'  Total: {len(vulns_net)}')\nprint(f'  Active: {vulns_net[\"is_active\"].sum()} ({vulns_net[\"is_active\"].sum()/len(vulns_net)*100:.1f}%)')\n\nprint(f'\\nAI vs NON-AI:')\nprint(f'  AI vulns: {vulns_net[\"is_ai_lib\"].sum()}')\nprint(f'  Non-AI vulns: {(~vulns_net[\"is_ai_lib\"]).sum()}')\n\nprint(f'\\nTIME-TO-FIX (median):')\nprint(f'  AI: {vulns_net[vulns_net[\"is_ai_lib\"]][\"time_to_fix_from_disclosure_days\"].median():.1f} days')\nprint(f'  Non-AI: {vulns_net[~vulns_net[\"is_ai_lib\"]][\"time_to_fix_from_disclosure_days\"].median():.1f} days')\n\nprint(f'\\nTOP 5 MOST IMPORTANT:')\ntop5 = metrics_df.nlargest(5, 'in_degree')[['package', 'in_degree']]\nfor _, row in top5.iterrows():\n    print(f'  {row[\"package\"]}: {row[\"in_degree\"]:.0f} dependents')\n\nprint('\\n' + '='*80)\nprint('\u2705 ANALYSIS COMPLETE!')\nprint('='*80)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}