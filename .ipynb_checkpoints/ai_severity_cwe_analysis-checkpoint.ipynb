{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AI 1-hop severity graph and CWE influence analysis\n",
        "\n",
        "This notebook builds:\n",
        "- AI 1-hop dependency graph with nodes colored by severity (from merged timeline)\n",
        "- CWE influence networks and AI-focused CWE propagation metrics\n",
        "\n",
        "Prerequisites:\n",
        "- `python_dependencies_edges.csv` in project root\n",
        "- `outputs/summaries/top_pypi_snyk_timeline_merged.csv` generated by the merge script\n",
        "- Optional for AI CWE: `outputs/top_pypi_snyk_timeline_20221112_20251112.csv`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import os, numpy as np\n",
        "import pandas as pd, networkx as nx, matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "os.makedirs('outputs/plots', exist_ok=True)\n",
        "\n",
        "# Helper to normalize package/node names\n",
        "def normalize_pkg(name: str) -> str:\n",
        "    return str(name).strip().lower().replace('_', '-')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define AI libraries (edit this list for your focus set)\n",
        "AI_LIBS = [\n",
        "    'torch', 'transformers', 'langchain', 'llama-index', 'openai',\n",
        "    'gradio', 'fastapi', 'mlflow', 'pytorch-lightning', 'tensorflow',\n",
        "]\n",
        "print(f\"AI_LIBS: {len(AI_LIBS)} items\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AI 1-hop severity graph (AI highlighted, all nodes colored by severity)\n",
        "\n",
        "# Force rebuild of globals if they exist (safe when rerunning)\n",
        "for v in ['DG_all','UG_all','pkg_sev','color_map']:\n",
        "    if v in globals():\n",
        "        del globals()[v]\n",
        "\n",
        "# 1) Build graph from dependency edges\n",
        "if not os.path.exists('python_dependencies_edges.csv'):\n",
        "    raise SystemExit('python_dependencies_edges.csv not found')\n",
        "edges = pd.read_csv('python_dependencies_edges.csv')\n",
        "edges['source'] = edges['source'].astype(str).map(normalize_pkg)\n",
        "edges['target'] = edges['target'].astype(str).map(normalize_pkg)\n",
        "DG_all = nx.DiGraph(); DG_all.add_edges_from(edges[['source','target']].itertuples(index=False, name=None))\n",
        "UG_all = DG_all.to_undirected()\n",
        "\n",
        "# 2) Build package -> severity map from merged timeline\n",
        "pkg_sev = {}\n",
        "vuln_path = 'outputs/summaries/top_pypi_snyk_timeline_merged.csv'\n",
        "if not os.path.exists(vuln_path):\n",
        "    # Fallback to base timeline\n",
        "    vuln_path = 'outputs/top_pypi_snyk_timeline_20231101_20251101.csv'\n",
        "\n",
        "if os.path.exists(vuln_path):\n",
        "    vulns = pd.read_csv(vuln_path)\n",
        "    if 'package' in vulns.columns:\n",
        "        vulns['package'] = vulns['package'].astype(str).map(normalize_pkg)\n",
        "    if 'severity' in vulns.columns:\n",
        "        vulns['severity'] = vulns['severity'].astype(str).str.lower().fillna('unknown')\n",
        "        sev_rank = {'low':1,'medium':2,'moderate':2,'high':3,'critical':4}\n",
        "        vulns['sev_rank'] = vulns['severity'].map(lambda s: sev_rank.get(s,0))\n",
        "        agg = vulns.groupby('package', as_index=False)['sev_rank'].max()\n",
        "        inv = {v:k for k,v in sev_rank.items()}\n",
        "        agg['severity_max'] = agg['sev_rank'].map(lambda r: inv.get(r,'unknown'))\n",
        "        pkg_sev = dict(zip(agg['package'], agg['severity_max']))\n",
        "\n",
        "color_map = {'critical':'#d73027','high':'#fc8d59','medium':'#fee08b','moderate':'#fee08b','low':'#91bfdb','unknown':'#bdbdbd'}\n",
        "\n",
        "# AI nodes + 1 hop neighbors\n",
        "ai_nodes = {normalize_pkg(p) for p in AI_LIBS if normalize_pkg(p) in UG_all}\n",
        "ai_focus = set(ai_nodes)\n",
        "for n in list(ai_nodes):\n",
        "    if n in UG_all:\n",
        "        ai_focus.update(UG_all.neighbors(n))\n",
        "\n",
        "H = UG_all.subgraph(ai_focus).copy()\n",
        "if H.number_of_nodes() == 0:\n",
        "    print('Empty AI 1-hop graph — check AI_LIBS and names present in dependency edges')\n",
        "else:\n",
        "    # Largest connected component for readability\n",
        "    comps = sorted(nx.connected_components(H), key=len, reverse=True)\n",
        "    H = H.subgraph(comps[0]).copy()\n",
        "\n",
        "    # Spring layout\n",
        "    k = 1/np.sqrt(max(H.number_of_nodes(),1))\n",
        "    pos = nx.spring_layout(H, k=k*3, iterations=450, seed=23)\n",
        "\n",
        "    # Sizes by in-degree (influence)\n",
        "    indeg = dict(DG_all.in_degree(H.nodes()))\n",
        "    base = np.array([max(1, indeg.get(n,0)) for n in H.nodes()])\n",
        "    p95 = np.percentile(base, 95) if np.any(base) else 1.0\n",
        "    sizes = (base/p95 * 500).clip(10, 900)\n",
        "\n",
        "    # AI highlight\n",
        "    ai_set = set(ai_nodes) & set(H.nodes())\n",
        "    sizes_ai  = [(max(1, indeg.get(n,0))/p95 * 900) for n in ai_set]\n",
        "    sizes_ai  = np.clip(sizes_ai, 40, 1400)\n",
        "\n",
        "    # Colors by severity\n",
        "    def sev(n):\n",
        "        s = str(pkg_sev.get(normalize_pkg(n), 'unknown')).lower()\n",
        "        return 'medium' if s == 'moderate' else s\n",
        "\n",
        "    # Draw\n",
        "    fig, ax = plt.subplots(1,1, figsize=(18,14))\n",
        "    nx.draw_networkx_edges(H, pos, ax=ax, width=0.35, alpha=0.14, edge_color='#9e9e9e')\n",
        "\n",
        "    ctx_nodes = [n for n in H.nodes() if n not in ai_set]\n",
        "    idx = {n:i for i,n in enumerate(H.nodes())}\n",
        "    ctx_sizes  = [sizes[idx[n]] for n in ctx_nodes]\n",
        "    ctx_colors = [color_map.get(sev(n), color_map['unknown']) for n in ctx_nodes]\n",
        "    nx.draw_networkx_nodes(H, pos, nodelist=ctx_nodes, node_size=ctx_sizes, node_color=ctx_colors,\n",
        "                           edgecolors='#666666', linewidths=0.25, alpha=0.95, ax=ax)\n",
        "\n",
        "    nx.draw_networkx_nodes(H, pos, nodelist=list(ai_set), node_size=sizes_ai,\n",
        "                           node_color=[color_map.get(sev(n), color_map['unknown']) for n in ai_set],\n",
        "                           edgecolors='black', linewidths=0.9, alpha=0.98, ax=ax)\n",
        "\n",
        "    # Label main AI hubs\n",
        "    ai_hubs = sorted([(n, indeg.get(n,0)) for n in ai_set], key=lambda x: x[1], reverse=True)[:25]\n",
        "    labels = {n:n for n,_ in ai_hubs}\n",
        "    nx.draw_networkx_labels(H, pos, labels=labels, font_size=9, font_weight='bold', ax=ax)\n",
        "\n",
        "    legend_handles = [\n",
        "        Patch(color=color_map['critical'], label='critical'),\n",
        "        Patch(color=color_map['high'],     label='high'),\n",
        "        Patch(color=color_map['medium'],   label='medium'),\n",
        "        Patch(color=color_map['low'],      label='low'),\n",
        "        Patch(facecolor='white', edgecolor='black', label='AI (bold border)'),\n",
        "    ]\n",
        "    ax.legend(handles=legend_handles, title='Legend', frameon=False, loc='lower left')\n",
        "\n",
        "    subtitle = f\"Nodes={H.number_of_nodes()}  Edges={H.number_of_edges()}  AI nodes={len(ai_set)}\"\n",
        "    ax.set_title(f\"AI packages — 1-hop (all dependencies colored by severity)\\n{subtitle}\", fontsize=14, fontweight='bold')\n",
        "    ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/plots/dependency_severity_ai_1hop_colored.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CWE influence network (merged timeline window)\n",
        "\n",
        "This section builds a directed CWE co-occurrence graph from the merged timeline, filters by date, and computes centrality metrics and plots.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "CWE_SOURCE_CSV = 'outputs/summaries/top_pypi_snyk_timeline_merged.csv'\n",
        "DATE_START = '2023-11-01'\n",
        "DATE_END   = '2025-11-01'\n",
        "TOP_K_CWE  = 30\n",
        "MIN_PAIR_COUNT = 3\n",
        "MIN_COND_P = 0.15\n",
        "\n",
        "import os, re, pandas as pd, numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from itertools import combinations\n",
        "from collections import Counter\n",
        "\n",
        "print(f'Loading {CWE_SOURCE_CSV}')\n",
        "df_src = pd.read_csv(CWE_SOURCE_CSV)\n",
        "for col in ['disclosed_date','first_affected_date','mitigation_date']:\n",
        "    if col in df_src.columns:\n",
        "        df_src[col] = pd.to_datetime(df_src[col], errors='coerce')\n",
        "\n",
        "mask = (df_src['disclosed_date'] >= pd.to_datetime(DATE_START)) & (df_src['disclosed_date'] <= pd.to_datetime(DATE_END))\n",
        "df = df_src.loc[mask].copy()\n",
        "print(f'Rows after date filter: {len(df)}')\n",
        "\n",
        "def parse_cwes(val):\n",
        "    if pd.isna(val):\n",
        "        return []\n",
        "    parts = re.split(r'[;\\,\\s]+', str(val))\n",
        "    out = []\n",
        "    for p in parts:\n",
        "        p = p.strip()\n",
        "        if not p: continue\n",
        "        up = p.upper()\n",
        "        if up.startswith('CWE-'): out.append(up)\n",
        "    return list(dict.fromkeys(out))\n",
        "\n",
        "if 'cwes' in df.columns:\n",
        "    df['cwe_list'] = df['cwes'].apply(parse_cwes)\n",
        "elif 'CWE' in df.columns:\n",
        "    df['cwe_list'] = df['CWE'].apply(parse_cwes)\n",
        "else:\n",
        "    df['cwe_list'] = [[] for _ in range(len(df))]\n",
        "\n",
        "all_cwes = [c for lst in df['cwe_list'] for c in lst]\n",
        "freq = Counter(all_cwes)\n",
        "top_cwes = [c for c,_ in freq.most_common(TOP_K_CWE)]\n",
        "print(f'Top-K CWE size: {len(top_cwes)}')\n",
        "\n",
        "pair_count = Counter()\n",
        "co_rows = 0\n",
        "for lst in df['cwe_list']:\n",
        "    lst = [c for c in lst if c in top_cwes]\n",
        "    if len(lst) >= 2:\n",
        "        co_rows += 1\n",
        "        seen = sorted(set(lst))\n",
        "        for i in range(len(seen)):\n",
        "            for j in range(i+1, len(seen)):\n",
        "                a, b = seen[i], seen[j]\n",
        "                pair_count[(a,b)] += 1\n",
        "                pair_count[(b,a)] += 1\n",
        "print(f'Rows with >=2 CWEs: {co_rows}')\n",
        "\n",
        "G_cwe = nx.DiGraph()\n",
        "for c in top_cwes:\n",
        "    G_cwe.add_node(c, freq=freq[c])\n",
        "\n",
        "for (a,b), cnt in pair_count.items():\n",
        "    if freq[a] > 0:\n",
        "        p = cnt / freq[a]\n",
        "        if cnt >= MIN_PAIR_COUNT and p >= MIN_COND_P:\n",
        "            G_cwe.add_edge(a, b, weight=p, count=cnt)\n",
        "\n",
        "print(f'CWE graph: nodes={G_cwe.number_of_nodes()} edges={G_cwe.number_of_edges()}')\n",
        "os.makedirs('outputs/summaries', exist_ok=True)\n",
        "\n",
        "# Centralities and communities\n",
        "try:\n",
        "    pr = nx.pagerank(G_cwe, weight='weight')\n",
        "except Exception:\n",
        "    pr = {n:0 for n in G_cwe.nodes()}\n",
        "out_w = {n: sum(d.get('weight',1.0) for _,_,d in G_cwe.out_edges(n, data=True)) for n in G_cwe.nodes()}\n",
        "in_w  = {n: sum(d.get('weight',1.0) for _,_,d in G_cwe.in_edges(n, data=True)) for n in G_cwe.nodes()}\n",
        "UG = G_cwe.to_undirected()\n",
        "try:\n",
        "    btw = nx.betweenness_centrality(UG, weight='weight', normalized=True)\n",
        "except Exception:\n",
        "    btw = {n:0 for n in G_cwe.nodes()}\n",
        "from networkx.algorithms import community\n",
        "comms = list(community.greedy_modularity_communities(UG, weight='weight')) if UG.number_of_edges()>0 else []\n",
        "node2comm = {}\n",
        "for i, comm in enumerate(comms):\n",
        "    for n in comm:\n",
        "        node2comm[n] = i\n",
        "rows = []\n",
        "for n in G_cwe.nodes():\n",
        "    rows.append({'cwe':n,'freq':G_cwe.nodes[n].get('freq',0),'pagerank':pr.get(n,0),\n",
        "                 'out_strength':out_w.get(n,0),'in_strength':in_w.get(n,0),'betweenness':btw.get(n,0),\n",
        "                 'community':node2comm.get(n,-1)})\n",
        "metrics_df_cwe = pd.DataFrame(rows).sort_values(['pagerank','out_strength'], ascending=False)\n",
        "metrics_df_cwe.to_csv('outputs/summaries/cwe_metrics.csv', index=False)\n",
        "# reach\n",
        "reach = {}\n",
        "for n in G_cwe.nodes():\n",
        "    seen=set(); stack=[n]\n",
        "    while stack:\n",
        "        u=stack.pop()\n",
        "        for v in G_cwe.successors(u):\n",
        "            if v not in seen:\n",
        "                seen.add(v); stack.append(v)\n",
        "    reach[n]=len(seen)\n",
        "metrics_df_cwe['reach_nodes']=metrics_df_cwe['cwe'].map(reach)\n",
        "metrics_df_cwe.to_csv('outputs/summaries/cwe_metrics.csv', index=False)\n",
        "print(metrics_df_cwe.head())\n",
        "\n",
        "# Plots (network, heatmap, curves)\n",
        "pos = nx.spring_layout(G_cwe, weight='weight', seed=42, k=2) if G_cwe.number_of_edges()>0 else {n:(i,0) for i,n in enumerate(G_cwe.nodes())}\n",
        "num_c = (max(node2comm.values())+1) if node2comm else 1\n",
        "cmap = plt.cm.tab20(np.linspace(0,1,max(1,num_c)))\n",
        "node_color=[cmap[node2comm.get(n,0)] for n in G_cwe.nodes()]\n",
        "node_size=[1000*(metrics_df_cwe.set_index('cwe').loc[n,'pagerank']+1e-4) for n in G_cwe.nodes()]\n",
        "edge_w=[d.get('weight',0.1)*3 for _,_,d in G_cwe.edges(data=True)]\n",
        "fig, ax = plt.subplots(1,1, figsize=(18,12))\n",
        "nx.draw_networkx_edges(G_cwe,pos,alpha=0.25,width=edge_w,arrows=True,arrowstyle='-|>',arrowsize=10,edge_color='gray',ax=ax)\n",
        "nx.draw_networkx_nodes(G_cwe,pos,node_color=node_color,node_size=node_size,edgecolors='black',linewidths=1.2,ax=ax)\n",
        "labels={row['cwe']:row['cwe'] for _,row in metrics_df_cwe.head(12).iterrows()}\n",
        "nx.draw_networkx_labels(G_cwe,pos,labels=labels,font_size=10,font_weight='bold',ax=ax)\n",
        "ax.set_title('CWE Influence Network (directed, weight=P(B|A))',fontsize=16,fontweight='bold'); ax.axis('off')\n",
        "plt.tight_layout(); plt.savefig('outputs/plots/cwe_influence_network.png',dpi=300,bbox_inches='tight'); plt.show()\n",
        "# Heatmap top 15\n",
        "N=min(15,len(metrics_df_cwe)); order=metrics_df_cwe.head(N)['cwe'].tolist();\n",
        "cond=np.zeros((N,N))\n",
        "for i,a in enumerate(order):\n",
        "    for j,b in enumerate(order):\n",
        "        if G_cwe.has_edge(a,b): cond[i,j]=G_cwe.edges[a,b].get('weight',0)\n",
        "fig, ax = plt.subplots(1,1, figsize=(12,10)); sns.heatmap(cond,xticklabels=order,yticklabels=order,cmap='YlOrRd',cbar_kws={'label':'P(B|A)'},ax=ax); plt.xticks(rotation=45,ha='right'); ax.set_title('CWE conditional probability (top 15)',fontsize=14,fontweight='bold'); plt.tight_layout(); plt.savefig('outputs/plots/cwe_conditional_heatmap.png',dpi=300,bbox_inches='tight'); plt.show()\n",
        "# Curves\n",
        "vals_out=np.sort(metrics_df_cwe['out_strength'].values)[::-1]; vals_in=np.sort(metrics_df_cwe['in_strength'].values)[::-1]\n",
        "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(16,6)); ax1.scatter(range(len(vals_out)),vals_out,c='#e74c3c',s=30); ax1.set_title('CWE influence (sorted out-strength)'); ax1.grid(alpha=0.3); ax2.scatter(range(len(vals_in)),vals_in,c='#7ea9e1',s=30); ax2.set_title('CWE dependency (sorted in-strength)'); ax2.grid(alpha=0.3); plt.tight_layout(); plt.savefig('outputs/plots/cwe_influence_dependency_curves.png',dpi=300,bbox_inches='tight'); plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AI-focused CWE propagation and P-Impact\n",
        "\n",
        "This section focuses on AI libraries only and computes CWE influence and dependency reach to derive P-Impact.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inputs\n",
        "SRC_TIMELINE = 'outputs/top_pypi_snyk_timeline_20221112_20251112.csv'\n",
        "DEPS_CSV    = 'python_dependencies_edges.csv'\n",
        "DATE_START  = '2023-11-01'\n",
        "DATE_END    = '2025-11-01'\n",
        "TOP_K_CWE   = 30\n",
        "MIN_PAIR    = 3\n",
        "MIN_P       = 0.10\n",
        "\n",
        "import pandas as pd, numpy as np, re, networkx as nx, matplotlib.pyplot as plt, seaborn as sns\n",
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "\n",
        "# Load & filter timeline\n",
        "if not os.path.exists(SRC_TIMELINE):\n",
        "    raise SystemExit(f'{SRC_TIMELINE} not found')\n",
        "if not os.path.exists(DEPS_CSV):\n",
        "    raise SystemExit(f'{DEPS_CSV} not found')\n",
        "\n",
        "df = pd.read_csv(SRC_TIMELINE)\n",
        "for col in ['disclosed_date','first_affected_date','mitigation_date']:\n",
        "    if col in df.columns: df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "mask = (df['disclosed_date']>=pd.to_datetime(DATE_START)) & (df['disclosed_date']<=pd.to_datetime(DATE_END))\n",
        "df = df.loc[mask].copy()\n",
        "df['package_lower'] = df['package'].astype(str).str.lower()\n",
        "\n",
        "# CWE parsing\n",
        "def parse_cwes(val):\n",
        "    if pd.isna(val): return []\n",
        "    out=[]\n",
        "    for p in re.split(r'[;\\,\\s]+', str(val)):\n",
        "        up=p.strip().upper()\n",
        "        if up.startswith('CWE-'): out.append(up)\n",
        "    return list(dict.fromkeys(out))\n",
        "\n",
        "cwe_col = 'cwes' if 'cwes' in df.columns else ('CWE' if 'CWE' in df.columns else None)\n",
        "df['cwe_list'] = df[cwe_col].apply(parse_cwes) if cwe_col else [[] for _ in range(len(df))]\n",
        "\n",
        "# Build CWE influence graph (directed co-occur)\n",
        "all_cwes=[c for lst in df['cwe_list'] for c in lst]\n",
        "freq=Counter(all_cwes)\n",
        "top_cwes=[c for c,_ in freq.most_common(TOP_K_CWE)]\n",
        "pair=Counter()\n",
        "for lst in df['cwe_list']:\n",
        "    lst=[c for c in lst if c in top_cwes]\n",
        "    if len(lst)>=2:\n",
        "        seen=sorted(set(lst))\n",
        "        for i in range(len(seen)):\n",
        "            for j in range(i+1,len(seen)):\n",
        "                a,b=seen[i],seen[j]; pair[(a,b)]+=1; pair[(b,a)]+=1\n",
        "G_cwe=nx.DiGraph()\n",
        "for c in top_cwes: G_cwe.add_node(c, freq=freq[c])\n",
        "for (a,b),cnt in pair.items():\n",
        "    if freq[a]>0:\n",
        "        p=cnt/freq[a]\n",
        "        if cnt>=MIN_PAIR and p>=MIN_P: G_cwe.add_edge(a,b,weight=p,count=cnt)\n",
        "try: pr = nx.pagerank(G_cwe, weight='weight')\n",
        "except: pr={n:0 for n in G_cwe.nodes()}\n",
        "# normalize PR to [0,1]\n",
        "if pr: \n",
        "    arr=np.array(list(pr.values())); mn, mx = arr.min(), arr.max();\n",
        "    pr_norm={k: (v-mn)/(mx-mn+1e-12) for k,v in pr.items()}\n",
        "else:\n",
        "    pr_norm={n:0 for n in G_cwe.nodes()}\n",
        "\n",
        "# AI libs only rows\n",
        "ai_set = {p.lower() for p in AI_LIBS}\n",
        "ai_df = df[df['package_lower'].isin(ai_set)].copy()\n",
        "# per-package CWE influence score (sum PR of its CWEs)\n",
        "def influence_for(lst): return float(np.sum([pr_norm.get(c,0) for c in lst]))\n",
        "pkg_infl = (ai_df.groupby('package_lower')['cwe_list']\n",
        "            .apply(lambda col: influence_for([c for lst in col for c in lst]))\n",
        "            .reset_index().rename(columns={'cwe_list':'cwe_influence'}))\n",
        "\n",
        "# Dependency graph and reverse reach to dependents\n",
        "deps = pd.read_csv(DEPS_CSV)\n",
        "deps['source']=deps['source'].astype(str).str.lower(); deps['target']=deps['target'].astype(str).str.lower()\n",
        "DG = nx.DiGraph(); DG.add_edges_from(deps[['source','target']].itertuples(index=False,name=None))\n",
        "RG = DG.reverse(copy=True)  # edges: lib -> dependents\n",
        "\n",
        "# reach per AI package\n",
        "reach_rows=[]\n",
        "from collections import deque\n",
        "for pkg in sorted(set(pkg_infl['package_lower'])):\n",
        "    if pkg not in RG: reach_rows.append((pkg,0,0.0)); continue\n",
        "    seen=set([pkg]); q=deque([(pkg,0)]); total=0; dsum=0\n",
        "    while q:\n",
        "        u,d=q.popleft()\n",
        "        for v in RG.successors(u):\n",
        "            if v not in seen:\n",
        "                seen.add(v); q.append((v,d+1)); total+=1; dsum+=d+1\n",
        "    avg_depth = (dsum/total) if total>0 else 0.0\n",
        "    reach_rows.append((pkg,total,avg_depth))\n",
        "reach_df = pd.DataFrame(reach_rows, columns=['package_lower','dependent_reach','avg_depth'])\n",
        "\n",
        "# Merge and compute P-Impact\n",
        "out = pkg_infl.merge(reach_df, on='package_lower', how='left')\n",
        "out['dependent_reach']=out['dependent_reach'].fillna(0)\n",
        "# normalize reach to [0,1]\n",
        "if len(out)>0:\n",
        "    r=out['dependent_reach'].astype(float); out['reach_norm']=(r-r.min())/(r.max()-r.min()+1e-12)\n",
        "else:\n",
        "    out['reach_norm']=0.0\n",
        "out['p_impact'] = 0.5*out['cwe_influence'] + 0.5*out['reach_norm']\n",
        "out = out.sort_values('p_impact', ascending=False)\n",
        "out['package']=out['package_lower']\n",
        "os.makedirs('outputs/summaries', exist_ok=True)\n",
        "out[['package','cwe_influence','dependent_reach','avg_depth','p_impact']].to_csv('outputs/summaries/ai_cwe_impact.csv', index=False)\n",
        "print('Saved: outputs/summaries/ai_cwe_impact.csv')\n",
        "\n",
        "# Plots\n",
        "top = out.head(20)\n",
        "fig, ax = plt.subplots(1,1, figsize=(14,7))\n",
        "sns.barplot(data=top, x='p_impact', y='package', ax=ax, color='#8ecae6')\n",
        "ax.set_title('AI libs — P-Impact (CWE influence × dependency reach)', fontsize=14, fontweight='bold')\n",
        "ax.set_xlabel('P-Impact'); ax.set_ylabel('package')\n",
        "plt.tight_layout(); plt.savefig('outputs/plots/ai_pimpact_ranking.png', dpi=300, bbox_inches='tight'); plt.show()\n",
        "\n",
        "# Scatter reach vs influence\n",
        "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
        "sns.scatterplot(data=out, x='cwe_influence', y='dependent_reach', ax=ax, color='#e76f51')\n",
        "for _,row in top.iterrows():\n",
        "    ax.annotate(row['package'], (row['cwe_influence'], row['dependent_reach']), xytext=(5,5), textcoords='offset points', fontsize=8)\n",
        "ax.set_xlabel('CWE influence (sum PR)'); ax.set_ylabel('# dependents (reach)')\n",
        "ax.set_title('AI libs — Influence vs Reach', fontsize=12, fontweight='bold')\n",
        "plt.tight_layout(); plt.savefig('outputs/plots/ai_influence_vs_reach.png', dpi=300, bbox_inches='tight'); plt.show()\n",
        "\n",
        "# Print short answers\n",
        "print('Top 10 AI libs to fix (by P-Impact):')\n",
        "print(out.head(10)[['package','p_impact','dependent_reach','avg_depth','cwe_influence']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional AI CWE plots: SPOF scatter and out-degree ranking\n",
        "UG = G_cwe.to_undirected()\n",
        "try:\n",
        "    btw = nx.betweenness_centrality(UG, weight='weight', normalized=True)\n",
        "except Exception:\n",
        "    btw = {n:0 for n in G_cwe.nodes()}\n",
        "deg_cent = nx.degree_centrality(UG) if UG.number_of_nodes()>0 else {n:0 for n in G_cwe.nodes()}\n",
        "plot_df = []\n",
        "for n in G_cwe.nodes(): plot_df.append({'cwe':n,'betweenness':btw.get(n,0.0),'degree_centrality':deg_cent.get(n,0.0)})\n",
        "plot_df = pd.DataFrame(plot_df).sort_values('degree_centrality', ascending=False)\n",
        "\n",
        "# SPOF scatter\n",
        "fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
        "sns.scatterplot(data=plot_df, x='betweenness', y='degree_centrality', color='#ee6c4d', ax=ax)\n",
        "for _,row in plot_df.head(12).iterrows():\n",
        "    ax.annotate(row['cwe'], (row['betweenness'], row['degree_centrality']), xytext=(5,5), textcoords='offset points', fontsize=8)\n",
        "ax.set_title('AI CWEs — Single point of failure risk', fontsize=13, fontweight='bold')\n",
        "ax.set_xlabel('Betweenness Centrality'); ax.set_ylabel('Degree Centrality')\n",
        "plt.tight_layout(); plt.savefig('outputs/plots/ai_cwe_spof_scatter.png', dpi=300, bbox_inches='tight'); plt.show()\n",
        "\n",
        "# Out-degree ranking curve\n",
        "out_deg = [(n, G_cwe.out_degree(n)) for n in G_cwe.nodes()]\n",
        "out_deg = sorted(out_deg, key=lambda x: x[1], reverse=True)\n",
        "x = list(range(1, len(out_deg)+1)); y = [v for _,v in out_deg]\n",
        "fig, ax = plt.subplots(1,1, figsize=(14,6))\n",
        "ax.scatter(x, y, color='#e76f51')\n",
        "for i,(n,v) in enumerate(out_deg[:12]):\n",
        "    ax.annotate(n, (i+1, v), xytext=(5,5), textcoords='offset points', fontsize=8)\n",
        "ax.set_title('AI CWEs most vulnerable (out-degree)', fontsize=13, fontweight='bold')\n",
        "ax.set_xlabel('CWE (sorted by out-degree)'); ax.set_ylabel('Out-degree')\n",
        "plt.tight_layout(); plt.savefig('outputs/plots/ai_cwe_outdegree_curve.png', dpi=300, bbox_inches='tight'); plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
